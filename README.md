# ğŸ“¸ blip-pytorch-captioning

> **From Pixels to Captions â€” Building an Intelligent Image Captioning System with PyTorch and BLIP**

---

### ğŸ›  Project Overview

This repository combines the power of state-of-the-art deep learning models with real-world social media data to create a robust **image captioning system**.

We leverage:
- **Instagram Image Caption Dataset** ğŸ“·  
  [Instagram Images with Captions - Kaggle Dataset](https://www.kaggle.com/datasets/prithvijaunjale/instagram-images-with-captions?select=instagram_data2)
- **BLIP (Bootstrapped Language Image Pretraining) Model** ğŸ§   
  [Salesforce BLIP Large Model on HuggingFace](https://huggingface.co/Salesforce/blip-image-captioning-large)
- **PyTorch** âš™ï¸  
  to orchestrate the entire training, evaluation, and fine-tuning pipeline.

---

### ğŸ”¥ Key Highlights
- Efficiently pre-process and load Instagram images and captions.
- Fine-tune the **BLIP Large** model using **PyTorch**.
- Implement custom training and evaluation loops with metric tracking (BLEU, ROUGE).
- Support for 5-fold cross-validation and automatic best model saving.
- Mixed precision training (`torch.cuda.amp`) for optimal GPU memory usage.
- **Flexible Script Architecture**:
  - ğŸ““ **Jupyter Notebook Cell Scripts**:  
    Modular scripts designed for easy cell-by-cell execution and debugging.  
    Available inside the folder: `scripts/Jupyter Notebook Cell Scripts`
  - ğŸ’» **VS Code Scripts**:  
    Complete runnable `.py` scripts for seamless execution on VS Code or terminal.  
    Available inside the folder: `scripts/VS Code Scripts`

---

### ğŸ“ˆ Project Goal
To build an end-to-end image-to-text captioning system that can generate natural, engaging captions for unseen Instagram images, pushing the boundaries of visual storytelling.

---

### ğŸš€ Technologies Used
- PyTorch ğŸ
- Hugging Face Transformers ğŸ¤—
- Kaggle Datasets ğŸ“‚
- NLTK, ROUGE Metrics ğŸ“
- CUDA Acceleration âš¡

---

# âœ¨ Let's turn pixels into poetry!

---
